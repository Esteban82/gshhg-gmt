<HEAD>
<TITLE>Updating the GSHHG data set</TITLE>
</HEAD>
<BODY BACKGROUND="ffffff">
<CENTER><H1>Updating the GSHHG data set</H2></CENTER>
<H2>1. Background</H2>
Appendix K in the GMT Technical Reference and Cookbook describes some of the
steps that were taken to arrive at the GSHHG polygon data set.  Both the GSHHG
data files and the netCDF files binned_GSHHS_?.cdf used by GMT to plot coastlines,
where ? is one of the five resolutions <B>f</B>(ull), <B>h</B>(igh), <B>i</B>(ntermediate), <B>l</B>(ow), or
<B>c</B>(rude) are automatically created by reformatting the underlying, raw, data bases.
From time to time it has become necessary to make changes to the raw data files
and regenerate new GSHHG and GMT coastline files.  So far, this has happened when
<OL>
<LI>A problem has been found affecting one or more polygons, such as internal crossovers,
duplicate points, lack of closure, and triplets of points representing zero-area features.
<LI>External crossovers between two polygons, especially after the Douglas-Peucker decimation
tool has been used to reduce the full-resolution data down to smaller size.
<LI>A feature (e.g., a small island) was completely missing from the data base.
<LI>River mouths were poorly represented in the WVS data base and in some cases
led to small islands being interpreted as lakes.
</OL>
Because GSHHG derives from WVS (relative good resolution for shorelines) and WDBII (considerably
poorer resolution for lakes), quality concerns will often be raised when a user is making
maps of a particular region of the world in great detail.  In such cases a relatively minor
shortcoming on a regional or global scale becomes crippling in that the GMT-generated map is
simply too far off when compared to local quality shoreline data.  In such situations it may
be reasonable to replace all or portions of one or more raw polygons and then recreate the
derivative products.
<BR>
This page will describe the process that is required in order to do such "surgery".  Lacking
high-level GUI tools this procedure is not for the faint of heart.  The revised data set must
pass many tests in order for us to consider the improvements in the next GSHHG and GMT releases.
<H2>2. Entry Requirements</H2>
You will need to obtain the underlying GSHHG data files and software via svn.  Cd to a suitable
place and check out the gshhg repository via<P>
	svn co svn://gmtserver.soest.hawaii.edu/gshhg/trunk gshhg<P>

In the top dir you will find config.mk.orig.  Copy it to config.mk and make changes to this file
if you need to change where the GMT4 src directory is.  You can ignore the GSHHG_NEW_VERSION
unless you are the guru.
In gshhg/src are a bunch of C programs required to maintain GSHHG, and they all depend on GMT 4.
Make sure you have GMT 4 installed.  Compile and link everything with "make install".
In gshhg you will find the GSHHG and WDBII directories with subdirectories for each resolution;
these hold the master ASCII files of all the data.
<H2>Data Reformatting</H2>
The tools required work on binary versions of the files (GSHHS_?_polygons.b) derived from the
ASCII polygons.  Cd into the src directory, open the rebuild.sh script in an editor, and read there.
Follow instructions for creating the binary files from the current master ASCII files.
<H2>3a. Minor edits of a particular polygon</H2>
If you just need to delete a point, move a point, or perhaps add a point, etc. the simplest
way is to use the fixcoast.m Matlab script.  It does require Matlab.  Read the comments of the
script.  You will first have to extract the polygon you want to work on from the relevant *.b file
using polygon_extract.  Place this polygon.ID file in the same dir as fixcoast.m, and run
fixcoast (ID), where ID is the number.  Then do your edits following the script instructions.
When finished and you hit q, it will write out the edited polygon as polygon.ID.new, and you
can use that to replace that segment in the relevant master ASCII file.
<H2>3b. Replacing all or part of a particular polygon</H2>
These are the steps you are likely to take:
<OL>
<LI>Identify the ID(s) of the polygon(s) you wish to edit or replace.
To do this you need to run <U>polygon_id</U>.  It accepts the data base file and
a lon,lat point and returns the ID of the polygon that came closest to this
point as well as that minimum distance in meters.  By making a pscoast map
zoomed in on a detail of the feature to be replaced you should be able to
pick a point that uniquely finds the ID.  In general, polygons are sorted by
size so the ID = 0 represents the Eurasia-Africa polygon.
<LI>Extract the polygon(s) that matches the ID(s).  Given the ID(s), use <U>polygon_extract</U>
without the -M or -b options to pull out an ASCII file per polygon selected.
Polygons will be written to files called polygon.ID. Place these in a subdirectory called pol.
<LI>Make the changes.  This could simply mean replacing the relevant polygon.ID
file with the new data (obviously in the same file format), or it could mean manually
changing some of the points to address the problem.  The Matlab function fixcoast.m
may be of use to you.  If you need to replace a segment of the polygon, you should
identify the starting point on the line segment you want to insert into the
coastline (note it must also be oriented correctly first; as the line number increases
the "left" side of the line must be oriented towards land - if not use tac or tail -r to reverse
the segment) and write down the lon, lat.  Now find the line number in the coastline that is
closest to that point using, e.g. for the point (237.59,47.6745) on polygon.1, do
<P>
mapproject polygon.1 -G237.59/47.6745 | awk '{if ($3 < 200) print NR, $0}'
<P>
This will echo out all points within 200 m of the given coordinate in -G.
Edit polygon.1, goto the line in question.  Then, use zoom.sh to make a
plot of the area that shows each point as a dot.  Visually identify which
points to replace and where to insert the new segment, and then identify
those points in the polygon.1 file.  Remove and insert accordingly, and save
polygon.1.  Now move polygon.1 to the sub-directory pol.
<LI>Update the binary data base.  You will need the tool <U>polygon_update</U> which takes
four arguments: Name of old data base, an ascii list of  polygon IDs that should be
removed completely, an ascii list of IDs for polygons that have been modified, and
the name of the new data base.  One of the two ascii lists may be /dev/null.  The ascii
polygon files must reside in the subdiretory pol in the current directory.  The
new database should be given the next logical version number (e.g., v4.3).  You will
need to update the definition in the makefile so that reformatting commands further
down will work on your revised files and not the old ones!
Now, updated polygons will replace the older ones and polygon areas and regions are
reset.  If the polygon's handedness conflict with the specified level then the polygon
will be reversed (the level is assumed fixed).
</OL>
<H2>3c. Adding an entirely new polygon</H2>
If you want to add a missing feature such as a small island or lake you need
to take these steps:
<OL>
<LI>Run make binary first to make sure you have the latest native binary files.
	Take a note of the number of polygons per resolution.
<LI>Separate new polygons into files called polygon.*, with * being polygon IDs
	starting higher than what you found in the previous step.
<LI>Convert the ascii polygons to a binary sub-database using <U>polygon_restore</U>.
Specify the level of the polygon; region and area etc will be determined, and
polygons will be reversed if need be.  If the polygons have different levels (e.g.,
a mix of islands and lakes) you need to run this step for each level separately.
If several sub-databases are made you can simply concatenate them into one file using
the cat command.
<LI>Use <U>polygon_shrink</U> to generate the four lower resolutions from the full resolution
sub-database.
<LI>Merge the new sub-database and the previous database using <U>polygon_sort</U> which
will rearrange the polygons in order of size.  This will renumber the polygons as well.
Repeat for each resolution.
</OL>
<H2>4. Checking for consistency</H2>
Once you have a set of five revised polygon database you must check that they are internally consistent.
Run these checks on each resolution:
<OL>
<LI><U>polygon_consistency</U>: This tool will check for point duplicates, closure problems,
internal crossings (a polygon crossing itself), header conflicts, and 3-point sections
spanning zero-degree angle.  Study the report and if problems are found you must redo
some of the steps above.  Keep doing this until the report is clean.
<LI><U>polygon_xover</U>: Will check if any polygon is crossing any other polygon.  This test
takes many hours for the full data set.  If crossings are found you must examine the
pairs of polygons and manually determine how to modify coordinates to eliminate the
crossing.  Update the database using <U>polygon_update</U> and test again.
</OL>
<H2>5. Reordering and resetting polygon IDs, ancestor and parent IDs</H2>
After we modify the polygon files we need to go through several steps to
assure that the 5 resolutions are synced.  The steps are:
<OL>
	<LI><U>polygon_sort</U> is used to reorder the polygons so that they
		are sorted according to polygon area.  Depending on what changes
		you have made, this step will shuffle the ordering and renumber
		the polygons, including reassigning IDs if any polygons were removed.
		Run this step for all resolutions.
	<LI><U>polygon_findlevel</U> will recalculate the hierarchical levels of all
		polygons and reverse any, if needed, so that the handedness is maintained.
		Run this step for all resolutions.
	<LI><U>polygon_hierarchy</U> takes as argument the full resolution binary file
		and checks all other resolution files and determines which IDs a feature
		has in the various hierarchical levels.  It can then assign ancestor ID
		and check that everything is internally consistent.  This step takes hours.
	<LI><U>polygon_sync</U> will read the GSHHG-hierarchy.txt file produced by the
		previous step and update all the binary files.
	<LI></U>binary_to_ascii</U> will now need to be run for all affected resolutions
		to rewrite the ASCII versions of the data (since it is these that are under
		CVS control).
</OL>
These steps can be executed by using the rebuild.sh script in src.  Run it repeatedly by
giving it different task numbers; type rebuild.sh with no arguments to see the various tasks.
<H2>6. Regenerating new GSHHG and GMT coastline files</H2>
Once your new database files have been created and finalized in the res_? directories
and you have updated the VERSION definition in the makefile, you can reformat the data
into GSHHG and GMT netCDF files:
<UL>
<LI><B>GSHHG</B>:  Just run "make gshhs"; this will make the five GSHHG files.
<LI><B>GMT</B>: First update the nodes grids with "make nodes" (will take awhile).
Once that is finished you can make the netCDF binned data files using "make bin".
</UL>
<H2>5. Point of Contact</H2>
Questions regarding this entire procedure should be directed to <A HREF="mailto:pwessel@hawaii.edu">Paul Wessel</A>.
<P>
<HR>
<I>
Modified Thursday, June 6, 2013 by P. Wessel.</I>
</BODY>
</HTML>
